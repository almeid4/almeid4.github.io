
@inproceedings{roman_dynamic_2014,
	address = {Montreal, Canada},
	title = {Dynamic {Kernel} {Selection} {Criteria} for {Bayesian} {Optimization}},
	copyright = {All rights reserved},
	url = {https://bayesopt.github.io/papers/2014/paper13.pdf},
	urldate = {2015-02-15},
	booktitle = {{BayesOpt} 2014:  {NIPS} {Workshop} on {Bayesian} {Optimization}},
	author = {Roman, Ibai and Santana, Roberto and Mendiburu, Alexander and Lozano, Jose A.},
	year = {2014}
}

@inproceedings{roman_bayesian_2016,
	title = {Bayesian optimization for parameter tuning in evolutionary algorithms},
	copyright = {All rights reserved},
	url = {https://doi.org/10.1109/CEC.2016.7744410},
	doi = {10.1109/CEC.2016.7744410},
	booktitle = {{IEEE} {Congress} on {Evolutionary} {Computation}, {CEC} 2016, {Vancouver}, {BC}, {Canada}, {July} 24-29, 2016},
	author = {Roman, Ibai and Ceberio, Josu and Mendiburu, Alexander and Lozano, Jose A.},
	year = {2016},
	keywords = {Kernel, Bayesian optimization, Linear programming, Optimization, Bayes methods, Tuning, combinatorial mathematics, combinatorial optimization, continuous optimization, Evolutionary algorithms, evolutionary computation, hybrid kernel EDA, offline parameter tuning algorithm, parameter tuning, permutation flowshop scheduling problem, sequential design strategy, Sociology},
	pages = {4839--4845}
}

@article{roman_evolving_2019,
	title = {Evolving {Gaussian} {Process} kernels from elementary mathematical expressions},
	copyright = {All rights reserved},
	url = {http://arxiv.org/abs/1910.05173},
	abstract = {Choosing the most adequate kernel is crucial in many Machine Learning applications. Gaussian Process is a state-of-the-art technique for regression and classification that heavily relies on a kernel function. However, in the Gaussian Process literature, kernels have usually been either ad hoc designed, selected from a predefined set, or searched for in a space of compositions of kernels which have been defined a priori. In this paper, we propose a Genetic-Programming algorithm that represents a kernel function as a tree of elementary mathematical expressions. By means of this representation, a wider set of kernels can be modeled, where potentially better solutions can be found, although new challenges also arise. The proposed algorithm is able to overcome these difficulties and find kernels that accurately model the characteristics of the data. This method has been tested in several real-world time-series extrapolation problems, improving the state-of-the-art results while reducing the complexity of the kernels.},
	urldate = {2019-10-20},
	journal = {arXiv:1910.05173 [cs, stat]},
	author = {Roman, Ibai and Santana, Roberto and Mendiburu, Alexander and Lozano, Jose A.},
	month = oct,
	year = {2019},
	note = {arXiv: 1910.05173},
	keywords = {Statistics - Machine Learning, Computer Science - Machine Learning}
}

@inproceedings{roman_kernel_2015,
	title = {Kernel hautapen dinamikoa {Optimizazio} {Bayesiarrean}},
	copyright = {All rights reserved},
	booktitle = {I. {Ikergazte}: {Nazioarteko} ikerketa euskaraz. {Kongresuko} artikulu-bilduma},
	publisher = {UEU},
	author = {Roman, Ibai and Santana, Roberto and Mendiburu, Alexander and Lozano, Jose A.},
	year = {2015},
	pages = {842}
}

@article{roman_experimental_2019,
	title = {An {Experimental} {Study} in {Adaptive} {Kernel} {Selection} for {Bayesian} {Optimization}},
	volume = {7},
	copyright = {All rights reserved},
	issn = {2169-3536},
	url = {https://doi.org/10.1109/ACCESS.2019.2960498},
	doi = {10.1109/ACCESS.2019.2960498},
	abstract = {Bayesian Optimization has been widely used along with Gaussian Processes for solving expensive-to-evaluate black-box optimization problems. Overall, this approach has shown good results, and particularly for parameter tuning of machine learning algorithms. Nonetheless, Bayesian Optimization has to be also configured to achieve the best possible performance, being the selection of the kernel function a crucial choice. This paper investigates the convenience of adaptively changing the kernel function during the optimization process, instead of fixing it a priori. Six adaptive kernel selection strategies are introduced and tested in well-known synthetic and real-world optimization problems. In order to provide a more complete evaluation of the proposed kernel selection variants, two major kernel parameter setting approaches have been tested. According to our results, apart from having the advantage of removing the selection of the kernel out of the equation, adaptive kernel selection criteria show a better performance than fixed-kernel approaches.},
	journal = {IEEE Access},
	author = {Roman, Ibai and Santana, Roberto and Mendiburu, Alexander and Lozano, Jose A.},
	year = {2019},
	keywords = {Adaptive kernel selection, Bayesian optimization, Gaussian process, parameter tuning},
	pages = {184294--184302}
}

@inproceedings{roman_evolving_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Evolving {Gaussian} {Process} {Kernels} for {Translation} {Editing} {Effort} {Estimation}},
	copyright = {All rights reserved},
	isbn = {978-3-030-38629-0},
	url = {https://doi.org/10.1007/978-3-030-38629-0_25},
	doi = {10.1007/978-3-030-38629-0_25},
	abstract = {In many Natural Language Processing problems the combination of machine learning and optimization techniques is essential. One of these problems is estimating the effort required to improve, under direct human supervision, a text that has been translated using a machine translation method. Recent developments in this area have shown that Gaussian Processes can be accurate for post-editing effort prediction. However, the Gaussian Process kernel has to be chosen in advance, and this choice influences the quality of the prediction. In this paper, we propose a Genetic Programming algorithm to evolve kernels for Gaussian Processes. We show that the combination of evolutionary optimization and Gaussian Processes removes the need for a-priori specification of the kernel choice, and achieves predictions that, in many cases, outperform those obtained with fixed kernels.},
	language = {en},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Roman, Ibai and Santana, Roberto and Mendiburu, Alexander and Lozano, Jose A.},
	editor = {Matsatsinis, Nikolaos F. and Marinakis, Yannis and Pardalos, Panos},
	year = {2020},
	keywords = {Evolutionary search, Gaussian Processes, Genetic Programming, Kernel selection, Quality Estimation},
	pages = {304--318}
}

@inproceedings{roman_bayesian_2020,
	address = {Cham},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Bayesian {Optimization} {Approaches} for {Massively} {Multi}-modal {Problems}},
	copyright = {All rights reserved},
	isbn = {978-3-030-38629-0},
	url = {https://doi.org/10.1007/978-3-030-38629-0_31},
	doi = {10.1007/978-3-030-38629-0_31},
	abstract = {The optimization of massively multi-modal functions is a challenging task, particularly for problems where the search space can lead the optimization process to local optima. While evolutionary algorithms have been extensively investigated for these optimization problems, Bayesian Optimization algorithms have not been explored to the same extent. In this paper, we study the behavior of Bayesian Optimization as part of a hybrid approach for solving several massively multi-modal functions. We use well-known benchmarks and metrics to evaluate how different variants of Bayesian Optimization deal with multi-modality.},
	language = {en},
	booktitle = {Learning and {Intelligent} {Optimization}},
	publisher = {Springer International Publishing},
	author = {Roman, Ibai and Mendiburu, Alexander and Santana, Roberto and Lozano, Jose A.},
	editor = {Matsatsinis, Nikolaos F. and Marinakis, Yannis and Pardalos, Panos},
	year = {2020},
	keywords = {Bayesian Optimization, Gaussian processes, Multi-modal Optimization},
	pages = {383--397}
}

@inproceedings{roman_sentiment_2019,
	address = {Prague, Czech Republic},
	series = {{GECCO} '19},
	title = {Sentiment analysis with genetically evolved gaussian kernels},
	copyright = {All rights reserved},
	isbn = {978-1-4503-6111-8},
	url = {https://doi.org/10.1145/3321707.3321779},
	doi = {10.1145/3321707.3321779},
	abstract = {Sentiment analysis consists of evaluating opinions or statements based on text analysis. Among the methods used to estimate the degree to which a text expresses a certain sentiment are those based on Gaussian Processes. However, traditional Gaussian Processes methods use a predefined kernels with hyperparameters that can be tuned but whose structure can not be adapted. In this paper, we propose the application of Genetic Programming for the evolution of Gaussian Process kernels that are more precise for sentiment analysis. We use use a very flexible representation of kernels combined with a multi-objective approach that considers simultaneously two quality metrics and the computational time required to evaluate those kernels. Our results show that the algorithm can outperform Gaussian Processes with traditional kernels for some of the sentiment analysis tasks considered.},
	urldate = {2020-02-12},
	booktitle = {Proceedings of the {Genetic} and {Evolutionary} {Computation} {Conference}},
	publisher = {Association for Computing Machinery},
	author = {Roman, Ibai and Mendiburu, Alexander and Santana, Roberto and Lozano, Jose A.},
	month = jul,
	year = {2019},
	keywords = {genetic programming, gaussian processes, emotion analysis},
	pages = {1328--1337}
}
